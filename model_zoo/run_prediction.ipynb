{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U segmentation-models-pytorch albumentations --user\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import numpy as np\n",
    "# import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import linzhutils as lu\n",
    "from testdataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as albu\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/scratch/project_2007251/hel1996/'\n",
    "\n",
    "IMAGE_DIR = 'small_images'\n",
    "OUTPUT_DIR = 'output'\n",
    "\n",
    "lu.checkDir(os.path.join(DATA_DIR, OUTPUT_DIR))\n",
    "\n",
    "\n",
    "MASK_DIR = IMAGE_DIR\n",
    "\n",
    "img_dir = os.path.join(DATA_DIR, IMAGE_DIR)\n",
    "anno_dir = os.path.join(DATA_DIR, MASK_DIR)\n",
    "\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "    test_transform = [albu.PadIfNeeded(512, 512)]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best saved checkpoint\n",
    "model = torch.load('./models/kussi_FPN_efficientnet-b7_7_0.490.pth')\n",
    "ENCODER = 'efficientnet-b7'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "CLASSES = ['kussi']\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test dataset\n",
    "\n",
    "test_dataset = Dataset(\n",
    "    img_dir,\n",
    "    anno_dir,\n",
    "    augmentation=get_validation_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # helper function for data visualization\n",
    "# def visualize(**images):\n",
    "#     \"\"\"PLot images in one row.\"\"\"\n",
    "#     n = len(images)\n",
    "#     plt.figure(figsize=(16, 5))\n",
    "#     for i, (name, image) in enumerate(images.items()):\n",
    "#         plt.subplot(1, n, i + 1)\n",
    "#         plt.xticks([])\n",
    "#         plt.yticks([])\n",
    "#         plt.title(' '.join(name.split('_')).title())\n",
    "#         plt.imshow(image)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_masks(masks):\n",
    "    global file_num\n",
    "    name = f\"result_masks_{file_num}.npy\"\n",
    "    filename = f\"{os.path.join(DATA_DIR, OUTPUT_DIR, name)}\"\n",
    "    concatenated = np.concatenate(masks, axis=0)\n",
    "    reshaped = concatenated.reshape((-1, 512, 512))\n",
    "    np.save(filename, np.array(masks))\n",
    "    print(f\"Saved {filename}\")\n",
    "    file_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileList = []\n",
    "for i in test_dataset.ids:\n",
    "    fileList.append(os.path.join(img_dir, i))\n",
    "np.save(os.path.join(DATA_DIR, OUTPUT_DIR, 'file_list.npy'),np.array(fileList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 97/37566 [00:01<07:55, 78.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/project_2007251/hel1996/small_images/hel1996_00_10.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 113/37566 [00:04<1:00:58, 10.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/project_2007251/hel1996/small_images/hel1996_00_100.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 510/37566 [00:11<46:49, 13.19it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/project_2007251/hel1996/small_images/hel1996_00_50.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 717/37566 [00:16<37:31, 16.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/project_2007251/hel1996/small_images/hel1996_00_80.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1118/37566 [00:23<31:55, 19.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/project_2007251/hel1996/small_images/hel1996_100_110.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1199/37566 [00:24<08:34, 70.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/project_2007251/hel1996/small_images/hel1996_100_30.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1215/37566 [00:28<1:00:24, 10.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/project_2007251/hel1996/small_images/hel1996_100_40.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1421/37566 [00:33<36:50, 16.35it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/project_2007251/hel1996/small_images/hel1996_100_70.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1718/37566 [00:39<34:57, 17.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/project_2007251/hel1996/small_images/hel1996_10_10.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1925/37566 [00:43<35:54, 16.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/project_2007251/hel1996/small_images/hel1996_10_20.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 2084/37566 [00:46<13:03, 45.28it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m list_5120 \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28mlen\u001b[39m(fileList))):\n\u001b[0;32m----> 3\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mtest_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m img[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m512\u001b[39m):\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28mprint\u001b[39m(fileList[i])\n",
      "File \u001b[0;32m/projappl/project_2007251/ForestPatches/model_zoo/testdataset.py:49\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, i):\n\u001b[1;32m     47\u001b[0m \n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# read data\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimages_fps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m     51\u001b[0m     mask \u001b[38;5;241m=\u001b[39m image\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "list_5120 = []\n",
    "for i in tqdm(range(0,len(fileList))):\n",
    "    img = test_dataset[i]\n",
    "    if img[0].shape != (3, 512, 512):\n",
    "        print(fileList[i])\n",
    "        list_5120.append(fileList[i])\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 127/37566 [00:05<40:13, 15.51it/s]  /run/nvme/job_16473906/tmp/ipykernel_1085237/3843638613.py:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_tensor = torch.from_numpy(np.array(input_batch)).to(DEVICE)\n",
      "  0%|          | 127/37566 [00:05<26:52, 23.21it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (3,512,512) into shape (3,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m input_batch\u001b[38;5;241m.\u001b[39mappend(test_dataset[i][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(input_batch) \u001b[38;5;241m==\u001b[39m batch_size:\n\u001b[0;32m---> 16\u001b[0m     x_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# print(x_tensor.shape)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     pr_mask_batch \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_tensor)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mround()\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (3,512,512) into shape (3,)"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "image_batch = []\n",
    "input_batch = []\n",
    "mask_batch = []\n",
    "result_masks = []\n",
    "file_num = 0\n",
    "total_size = 0\n",
    "\n",
    "index = 0\n",
    "\n",
    "for i in tqdm(range(len(test_dataset))):\n",
    "    # image_batch.append(image[i])\n",
    "    input_batch.append(test_dataset[i][0])\n",
    "\n",
    "    if len(input_batch) == batch_size:\n",
    "        x_tensor = torch.from_numpy(np.array(input_batch)).to(DEVICE)\n",
    "        # print(x_tensor.shape)\n",
    "        pr_mask_batch = model.predict(x_tensor).squeeze().cpu().numpy().round()\n",
    "        result_masks.append(pr_mask_batch)\n",
    "        total_size += pr_mask_batch.nbytes\n",
    "        \n",
    "        index += 1\n",
    "        \n",
    "        # break\n",
    "        if total_size > 1e9:\n",
    "            save_masks(result_masks)\n",
    "            result_masks = []\n",
    "            total_size = 0\n",
    "\n",
    "        input_batch = []\n",
    "        mask_batch = []\n",
    "save_masks(result_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # helper function for data visualization\n",
    "# def to_image(x, **kwargs):\n",
    "#     if len(x.shape) == 3:\n",
    "#         return x.transpose(1, 2, 0)\n",
    "#     else: return x\n",
    "\n",
    "# def visualize_batch(img, msk):\n",
    "#     for i in range(img.shape[0]):\n",
    "#         plt.figure(figsize=(16, 5))\n",
    "#         plt.subplot(1, 2, 1)\n",
    "#         plt.imshow(to_image(img[i]))\n",
    "#         plt.subplot(1, 2, 2)\n",
    "#         plt.imshow(to_image(msk[0][i]))\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in image_batch:\n",
    "#     plt.figure()\n",
    "#     plt.imshow(to_image(i))\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = np.array(input_batch.copy())\n",
    "# msk = result_masks.copy()\n",
    "# visualize_batch(img, msk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(result_masks[0])):\n",
    "#     plt.figure()\n",
    "#     plt.imshow(result_masks[0][i])\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# for i in range(20):\n",
    "#     n = np.random.choice(len(test_dataset))\n",
    "\n",
    "#     t1 = time.time()\n",
    "    \n",
    "#     image_vis = test_dataset_vis[n][0].astype('uint8')\n",
    "#     image, gt_mask = test_dataset[n]\n",
    "\n",
    "#     gt_mask = gt_mask.squeeze()\n",
    "\n",
    "#     x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "#     pr_mask = model.predict(x_tensor)\n",
    "#     pr_mask = (pr_mask.squeeze().cpu().numpy().round())\n",
    "\n",
    "#     print(time.time()-t1)\n",
    "    \n",
    "#     visualize(image=image_vis,\n",
    "#               ground_truth_mask=gt_mask,\n",
    "#               predicted_mask=pr_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
